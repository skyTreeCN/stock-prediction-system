from anthropic import Anthropic
import pandas as pd
from typing import List, Dict
import json
import os
from datetime import datetime
from .config import get_model_id, get_active_model
from .pattern_matcher import load_classic_patterns, match_classic_patterns, pre_screen_stocks

class StockAnalyzer:
    """ä½¿ç”¨ Claude AI è¿›è¡Œè‚¡ç¥¨åˆ†æ"""

    def __init__(self, api_key: str, model: str = None):
        if not api_key:
            raise ValueError("API Keyä¸èƒ½ä¸ºç©º,è¯·è®¾ç½®ANTHROPIC_API_KEYç¯å¢ƒå˜é‡")

        self.client = Anthropic(api_key=api_key)

        # APIè°ƒç”¨ç»Ÿè®¡
        self.api_calls = 0
        self.api_errors = 0
        self.total_input_tokens = 0
        self.total_output_tokens = 0

        # ä»é…ç½®æ–‡ä»¶è·å–æ¨¡å‹ID
        if model is None:
            self.model = get_model_id()
            model_config = get_active_model()
            print(f"âœ… åˆå§‹åŒ– StockAnalyzer")
            print(f"   æ¨¡å‹: {model_config['name']} ({self.model})")
            print(f"   é¢„æœŸå‡†ç¡®ç‡: {model_config.get('accuracy_5p', 'N/A')}% (5ç§æ¨¡å¼)")
            print(f"   é€‚ç”¨åœºæ™¯: {model_config['use_case']}")
        else:
            self.model = model
            print(f"âš ï¸  ä½¿ç”¨è‡ªå®šä¹‰æ¨¡å‹: {self.model}")

    def analyze_rising_patterns(self, sample_data: pd.DataFrame) -> List[Dict]:
        """åˆ†æä¸Šæ¶¨æ¨¡å¼

        Args:
            sample_data: å†å²ä¸Šæ¶¨æ ·æœ¬æ•°æ®

        Returns:
            List of pattern dictionaries
        """
        # å‡†å¤‡æ•°æ®æ‘˜è¦
        data_summary = self._prepare_data_summary(sample_data)

        prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„è‚¡ç¥¨æŠ€æœ¯åˆ†æå¸ˆã€‚æˆ‘ç»™ä½ æä¾›äº†{len(sample_data)}ä¸ªå†å²ä¸Šæ¶¨æ¡ˆä¾‹ï¼ˆ3å¤©åæ”¶ç›˜ä»·ä¸Šæ¶¨â‰¥8%çš„è‚¡ç¥¨æ•°æ®ï¼‰ã€‚

è¯·æ·±å…¥åˆ†æè¿™äº›æ•°æ®ï¼Œæ€»ç»“å‡º8-12ç§å…·æœ‰ä»£è¡¨æ€§çš„ä¸Šæ¶¨æ¨¡å¼ç‰¹å¾ã€‚ç”±äºæ ·æœ¬é‡å¾ˆå¤§ï¼Œè¯·å°½é‡è¯†åˆ«å‡ºæ›´å¤šç»†åˆ†çš„æ¨¡å¼ç±»å‹ã€‚

æ•°æ®ç¤ºä¾‹ï¼ˆå‰20æ¡ï¼‰ï¼š
{data_summary}

è¯·ä»ä»¥ä¸‹è§’åº¦åˆ†æï¼š
1. Kçº¿å½¢æ€ç‰¹å¾ï¼ˆå¦‚é˜³çº¿è¿ç»­ã€å®ä½“å¤§å°ã€å½±çº¿ç‰¹å¾ç­‰ï¼‰
2. ä»·æ ¼å˜åŒ–å¹…åº¦ç‰¹å¾ï¼ˆæ¸©å’Œä¸Šæ¶¨ã€åŠ é€Ÿä¸Šæ¶¨ã€çˆ†å‘ä¸Šæ¶¨ç­‰ï¼‰
3. æˆäº¤é‡å˜åŒ–ç‰¹å¾ï¼ˆæ”¾é‡ã€ç¼©é‡ã€æ¸©å’Œæ”¾é‡ç­‰ï¼‰
4. æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾ï¼ˆå¦‚å‡çº¿ã€MACDã€è¶‹åŠ¿çº¿ç­‰å¯ä»¥ä»ä»·æ ¼è®¡ç®—ï¼‰
5. å¸‚åœºç¯å¢ƒç‰¹å¾ï¼ˆçªç ´ã€åå¼¹ã€è¶‹åŠ¿å»¶ç»­ç­‰ï¼‰

**é‡è¦ï¼šå¯¹äºæ¯ä¸ªæ¨¡å¼ï¼Œè¯·æä¾›ä¸€ä¸ªæœ€å…¸å‹çš„ç¤ºä¾‹è‚¡ç¥¨ä»£ç ï¼Œä»¥åŠè¯¥æ¨¡å¼çš„é‡åŒ–ç‰¹å¾ã€‚**

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
[
    {{
        "pattern_name": "æ¨¡å¼åç§°",
        "description": "æ¨¡å¼æè¿°",
        "characteristics": ["ç‰¹å¾1", "ç‰¹å¾2", "ç‰¹å¾3"],
        "example_stock_code": "ç¤ºä¾‹è‚¡ç¥¨ä»£ç ï¼ˆä»æ•°æ®ä¸­é€‰æ‹©æœ€å…¸å‹çš„ï¼‰",
        "highlight_description": {{
            "key_days": "å…³é”®Kçº¿çš„ç›¸å¯¹ä½ç½®æè¿°ï¼Œå¦‚'æœ€è¿‘3å¤©'ã€'ç¬¬85-89å¤©'",
            "key_features": ["éœ€è¦åœ¨Kçº¿å›¾ä¸Šæ ‡æ³¨çš„å…³é”®ç‚¹ï¼Œå¦‚'è¿ç»­3æ ¹é˜³çº¿'ã€'æˆäº¤é‡æ”¾å¤§2å€'"]
        }}
    }}
]

æ³¨æ„ï¼š
- è¯·è‡³å°‘è¿”å›8ç§æ¨¡å¼ï¼Œæœ€å¤š12ç§
- æ¯ç§æ¨¡å¼åº”æœ‰æ˜ç¡®åŒºåˆ†åº¦
- æ¨¡å¼åç§°è¦ç®€æ´ä¸“ä¸š
- example_stock_codeå¿…é¡»ä»æä¾›çš„æ•°æ®ä¸­é€‰æ‹©
- highlight_descriptionè¦å…·ä½“ï¼Œä¾¿äºåœ¨Kçº¿å›¾ä¸Šå¯è§†åŒ–
- åªè¿”å›JSONæ•°ç»„ï¼Œä¸è¦å…¶ä»–æ–‡å­—"""

        try:
            message = self.client.messages.create(
                model=self.model,
                max_tokens=4096,
                messages=[
                    {"role": "user", "content": prompt}
                ]
            )

            # æ›´æ–°APIç»Ÿè®¡
            self.api_calls += 1
            self.total_input_tokens += message.usage.input_tokens
            self.total_output_tokens += message.usage.output_tokens

            response_text = message.content[0].text
            print(f"[DEBUG] Claude å“åº”: {response_text[:200]}...")

            # æ¸…ç† markdown ä»£ç å—æ ‡è®°
            if response_text.strip().startswith('```'):
                # ç§»é™¤ ```json å’Œ ```
                lines = response_text.strip().split('\n')
                response_text = '\n'.join(lines[1:-1]) if len(lines) > 2 else response_text

            # è§£æJSON
            patterns = json.loads(response_text)
            return patterns

        except Exception as e:
            self.api_errors += 1
            print(f"Claude åˆ†æå¤±è´¥: {e}")
            print(f"[DEBUG] é”™è¯¯ç±»å‹: {type(e).__name__}")
            if hasattr(e, 'response'):
                print(f"[DEBUG] Response: {e.response}")
            return []

    def predict_stock_probability(
        self,
        stock_data: pd.DataFrame,
        patterns: List[Dict],
        batch_size: int = 50,
        use_pre_screening: bool = True,
        pattern_file: str = 'classic_patterns.json'
    ) -> List[Dict]:
        """æ‰¹é‡é¢„æµ‹è‚¡ç¥¨ä¸Šæ¶¨æ¦‚ç‡ï¼ˆæ”¯æŒç¨‹åºé¢„ç­›é€‰ï¼‰

        Args:
            stock_data: è‚¡ç¥¨æœ€è¿‘çš„Kçº¿æ•°æ®ï¼ˆæŒ‰è‚¡ç¥¨åˆ†ç»„ï¼‰
            patterns: å·²è¯†åˆ«çš„ä¸Šæ¶¨æ¨¡å¼
            batch_size: æ¯æ‰¹å¤„ç†çš„è‚¡ç¥¨æ•°é‡
            use_pre_screening: æ˜¯å¦ä½¿ç”¨ç¨‹åºé¢„ç­›é€‰ï¼ˆé™ä½æˆæœ¬ï¼‰
            pattern_file: ç»å…¸æ¨¡å¼å®šä¹‰æ–‡ä»¶è·¯å¾„

        Returns:
            List of predictions with probability
        """
        # æŒ‰è‚¡ç¥¨ä»£ç åˆ†ç»„
        grouped = stock_data.groupby('code')
        codes = list(grouped.groups.keys())

        # ç¨‹åºé¢„ç­›é€‰é˜¶æ®µ
        if use_pre_screening:
            print(f"\nğŸ” ç¨‹åºé¢„ç­›é€‰é˜¶æ®µ")
            print(f"   æ€»è‚¡ç¥¨æ•°: {len(codes)}")

            try:
                # åŠ è½½ç»å…¸æ¨¡å¼
                classic_patterns = load_classic_patterns(pattern_file)

                # å‡†å¤‡è‚¡ç¥¨Kçº¿æ•°æ®
                stocks_kline_data = {}
                for code in codes:
                    df = grouped.get_group(code).sort_values('date')
                    recent = df.tail(30)  # æœ€è¿‘30å¤©
                    kline_data = recent[['date', 'open', 'high', 'low', 'close', 'volume']].to_dict('records')
                    stocks_kline_data[code] = kline_data

                # ç¨‹åºé¢„ç­›é€‰
                candidate_codes = pre_screen_stocks(stocks_kline_data, classic_patterns)
                print(f"   ç­›é€‰åå€™é€‰: {len(candidate_codes)} åª")

                # å¦‚æœç­›é€‰åå¤ªå°‘ï¼Œå–å‰50ä¸ª
                if len(candidate_codes) < 10:
                    print(f"   âš ï¸  å€™é€‰å¤ªå°‘ï¼Œä½¿ç”¨å…¨éƒ¨è‚¡ç¥¨")
                    codes = codes[:50]
                else:
                    codes = candidate_codes[:50]  # æœ€å¤š50åª

            except Exception as e:
                print(f"   âš ï¸  é¢„ç­›é€‰å¤±è´¥ï¼Œä½¿ç”¨å…¨éƒ¨è‚¡ç¥¨: {e}")
                codes = codes[:50]
        else:
            print(f"\nğŸ” ç›´æ¥AIåˆ†æï¼ˆæœªä½¿ç”¨é¢„ç­›é€‰ï¼‰")
            codes = codes[:50]

        all_predictions = []

        # åˆ†æ‰¹å¤„ç†
        for i in range(0, len(codes), batch_size):
            batch_codes = codes[i:i + batch_size]
            batch_data = {code: grouped.get_group(code) for code in batch_codes}

            predictions = self._predict_batch(batch_data, patterns)
            all_predictions.extend(predictions)

        # æŒ‰æ¦‚ç‡æ’åº
        all_predictions.sort(key=lambda x: x['probability'], reverse=True)

        # ä¿å­˜é¢„æµ‹ç»“æœåˆ°æ•°æ®åº“
        from datetime import datetime
        prediction_date = datetime.now().strftime('%Y-%m-%d')

        for pred in all_predictions[:100]:
            try:
                self.db.save_prediction({
                    'stock_code': pred['code'],
                    'stock_name': pred.get('name', ''),
                    'prediction_date': prediction_date,
                    'matched_patterns': pred.get('matched_patterns', []),
                    'probability': pred['probability'],
                    'reasoning': pred.get('reasoning', '')
                })
            except Exception as e:
                print(f"ä¿å­˜é¢„æµ‹ç»“æœå¤±è´¥ {pred['code']}: {e}")

        return all_predictions[:100]  # è¿”å›å‰100ä¸ª

    def _predict_batch(self, batch_data: Dict[str, pd.DataFrame], patterns: List[Dict]) -> List[Dict]:
        """é¢„æµ‹ä¸€æ‰¹è‚¡ç¥¨"""

        # å‡†å¤‡æ‰¹é‡æ•°æ®æ‘˜è¦å’Œå…ƒæ•°æ®å­—å…¸
        batch_summary = []
        stock_metadata = {}  # å­˜å‚¨æ¯ä¸ªè‚¡ç¥¨çš„å…ƒæ•°æ®

        for code, df in batch_data.items():
            df_sorted = df.sort_values('date')
            recent = df_sorted.tail(30)  # æœ€è¿‘30å¤©ï¼ˆçº¦1ä¸ªæœˆï¼‰

            stock_name = df['name'].iloc[0] if 'name' in df.columns else code
            current_price = float(recent['close'].iloc[-1])
            last_date = str(recent['date'].iloc[-1])

            # ä¿å­˜å…ƒæ•°æ®
            stock_metadata[code] = {
                'name': stock_name,
                'current_price': current_price,
                'last_date': last_date
            }

            summary = {
                'code': code,
                'name': stock_name,
                'current_price': current_price,
                'last_date': last_date,
                'recent_data': recent[['date', 'open', 'close', 'high', 'low', 'volume']].to_dict('records')
            }
            batch_summary.append(summary)

        # å‡†å¤‡æ¨¡å¼æè¿°
        patterns_text = "\n".join([
            f"{i+1}. {p['pattern_name']}: {p['description']}"
            for i, p in enumerate(patterns)
        ])

        prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„è‚¡ç¥¨é¢„æµ‹åˆ†æå¸ˆã€‚

å·²çŸ¥çš„ä¸Šæ¶¨æ¨¡å¼ï¼š
{patterns_text}

ç°åœ¨æœ‰{len(batch_summary)}åªè‚¡ç¥¨çš„æœ€è¿‘æ•°æ®ï¼Œè¯·æ ¹æ®ä¸Šæ¶¨æ¨¡å¼åˆ†ææ¯åªè‚¡ç¥¨åœ¨æœªæ¥3å¤©ä¸Šæ¶¨çš„æ¦‚ç‡ã€‚

è‚¡ç¥¨æ•°æ®ï¼š
{json.dumps(batch_summary[:10], ensure_ascii=False, indent=2)}
{'...(è¿˜æœ‰æ›´å¤šè‚¡ç¥¨)' if len(batch_summary) > 10 else ''}

è¯·å¯¹æ¯åªè‚¡ç¥¨è¿›è¡Œåˆ†æï¼Œè¯„ä¼°å…¶ä¸ä¸Šæ¶¨æ¨¡å¼çš„ç›¸ä¼¼åº¦ï¼Œç»™å‡º0-100çš„ä¸Šæ¶¨æ¦‚ç‡è¯„åˆ†ã€‚

è¿”å›JSONæ ¼å¼ï¼š
[
    {{
        "code": "è‚¡ç¥¨ä»£ç ",
        "name": "è‚¡ç¥¨åç§°",
        "probability": 85.5,
        "reason": "ç®€è¦è¯´æ˜ç¬¦åˆå“ªäº›ç‰¹å¾"
    }}
]

æ³¨æ„ï¼š
- åªè¿”å›æ¦‚ç‡å¤§äº60çš„è‚¡ç¥¨
- probability æ˜¯0-100çš„æ•°å€¼
- reason ç®€çŸ­è¯´æ˜ï¼ˆä¸è¶…è¿‡50å­—ï¼‰
- åªè¿”å›JSONæ•°ç»„ï¼Œä¸è¦å…¶ä»–æ–‡å­—"""

        try:
            message = self.client.messages.create(
                model=self.model,
                max_tokens=4096,
                messages=[
                    {"role": "user", "content": prompt}
                ]
            )

            response_text = message.content[0].text

            # æå–JSONï¼ˆå¯èƒ½æœ‰markdownä»£ç å—ï¼‰
            if "```json" in response_text:
                response_text = response_text.split("```json")[1].split("```")[0]
            elif "```" in response_text:
                response_text = response_text.split("```")[1].split("```")[0]

            predictions = json.loads(response_text.strip())

            # åˆå¹¶å…ƒæ•°æ®åˆ°é¢„æµ‹ç»“æœ
            for pred in predictions:
                code = pred.get('code')
                if code in stock_metadata:
                    pred['current_price'] = stock_metadata[code]['current_price']
                    pred['last_date'] = stock_metadata[code]['last_date']
                    # ç¡®ä¿nameä¸€è‡´
                    if 'name' not in pred or not pred['name']:
                        pred['name'] = stock_metadata[code]['name']

            return predictions

        except Exception as e:
            print(f"é¢„æµ‹æ‰¹æ¬¡å¤±è´¥: {e}")
            return []

    def validate_patterns_sql(
        self,
        patterns: List[Dict],
        validation_data: pd.DataFrame,
        rise_threshold: float = 0.08
    ) -> List[Dict]:
        """ä½¿ç”¨SQLæ–¹æ³•éªŒè¯æ¨¡å¼ï¼ˆåŸºäºç‰¹å¾åŒ¹é…ï¼Œæ— é¢å¤–æˆæœ¬ï¼‰

        Args:
            patterns: AIè¯†åˆ«çš„æ¨¡å¼åˆ—è¡¨
            validation_data: éªŒè¯æ•°æ®é›†ï¼ˆåŒ…å«is_successå­—æ®µï¼‰
            rise_threshold: ä¸Šæ¶¨é˜ˆå€¼

        Returns:
            æ›´æ–°åçš„æ¨¡å¼åˆ—è¡¨ï¼ŒåŒ…å«validated_success_rateå­—æ®µ
        """
        print(f"\nğŸ“Š å¼€å§‹éªŒè¯æ¨¡å¼ï¼ˆSQLæ–¹æ³•ï¼‰")
        print(f"   éªŒè¯æ ·æœ¬æ•°: {len(validation_data)}")

        for pattern in patterns:
            pattern_name = pattern['pattern_name']
            characteristics = pattern.get('characteristics', [])

            # æ ¹æ®æ¨¡å¼ç‰¹å¾ç­›é€‰åŒ¹é…æ ·æœ¬
            matched_data = self._filter_by_characteristics(validation_data, characteristics)

            if len(matched_data) > 0:
                total_samples = len(matched_data)
                success_samples = matched_data['is_success'].sum()
                success_rate = (success_samples / total_samples) * 100
            else:
                total_samples = 0
                success_samples = 0
                success_rate = 0

            pattern['validated_success_rate'] = round(success_rate, 2)
            pattern['validation_sample_count'] = total_samples
            pattern['validation_date'] = datetime.now().strftime('%Y-%m-%d')

            print(f"   âœ“ {pattern_name}: {success_rate:.1f}% ({success_samples}/{total_samples})")

        return patterns

    def validate_patterns_ai(
        self,
        patterns: List[Dict],
        validation_data: pd.DataFrame,
        rise_threshold: float = 0.08
    ) -> List[Dict]:
        """ä½¿ç”¨AIæ–¹æ³•éªŒè¯æ¨¡å¼ï¼ˆæ›´ç²¾ç¡®ï¼Œæœ‰æˆæœ¬ï¼‰

        Args:
            patterns: AIè¯†åˆ«çš„æ¨¡å¼åˆ—è¡¨
            validation_data: éªŒè¯æ•°æ®é›†
            rise_threshold: ä¸Šæ¶¨é˜ˆå€¼

        Returns:
            æ›´æ–°åçš„æ¨¡å¼åˆ—è¡¨ï¼ŒåŒ…å«validated_success_rateå­—æ®µ
        """
        print(f"\nğŸ“Š å¼€å§‹éªŒè¯æ¨¡å¼ï¼ˆAIæ–¹æ³•ï¼‰")
        print(f"   éªŒè¯æ ·æœ¬æ•°: {len(validation_data)}")

        # å‡†å¤‡éªŒè¯æ•°æ®æ‘˜è¦ï¼ˆä½¿ç”¨å…¨éƒ¨éªŒè¯æ ·æœ¬ï¼‰
        validation_summary = self._prepare_data_summary(validation_data, limit=len(validation_data))

        for i, pattern in enumerate(patterns):
            pattern_name = pattern['pattern_name']
            description = pattern['description']
            characteristics = pattern['characteristics']

            prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„è‚¡ç¥¨æ¨¡å¼éªŒè¯åˆ†æå¸ˆã€‚

å·²è¯†åˆ«çš„æ¨¡å¼:
- åç§°: {pattern_name}
- æè¿°: {description}
- ç‰¹å¾: {', '.join(characteristics)}

éªŒè¯æ•°æ®ï¼ˆæœ€è¿‘1ä¸ªæœˆçš„å†å²æ ·æœ¬ï¼Œå…±{len(validation_data)}æ¡ï¼‰:
{validation_summary}

è¯·åˆ†æè¿™äº›éªŒè¯æ•°æ®ä¸­ï¼Œæœ‰å¤šå°‘æ¯”ä¾‹çš„æ ·æœ¬ç¬¦åˆä¸Šè¿°æ¨¡å¼ç‰¹å¾ã€‚

è¿”å›JSONæ ¼å¼:
{{
    "matched_count": åŒ¹é…è¯¥æ¨¡å¼çš„æ ·æœ¬æ•°é‡,
    "total_count": æ€»æ ·æœ¬æ•°,
    "success_rate": æˆåŠŸç‡ç™¾åˆ†æ¯”ï¼ˆ0-100ï¼‰,
    "analysis": "ç®€è¦åˆ†æ"
}}

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—ã€‚"""

            try:
                message = self.client.messages.create(
                    model=self.model,
                    max_tokens=500,
                    messages=[
                        {"role": "user", "content": prompt}
                    ]
                )

                response_text = message.content[0].text

                # æå–JSON
                if "```json" in response_text:
                    response_text = response_text.split("```json")[1].split("```")[0]
                elif "```" in response_text:
                    response_text = response_text.split("```")[1].split("```")[0]

                result = json.loads(response_text.strip())

                pattern['validated_success_rate'] = round(result['success_rate'], 2)
                pattern['validation_sample_count'] = result['total_count']
                pattern['validation_date'] = datetime.now().strftime('%Y-%m-%d')

                print(f"   âœ“ {pattern_name}: {result['success_rate']:.1f}% ({result['matched_count']}/{result['total_count']})")

            except Exception as e:
                print(f"   âœ— {pattern_name} éªŒè¯å¤±è´¥: {e}")
                # é™çº§åˆ°SQLæ–¹æ³•
                pattern['validated_success_rate'] = 0
                pattern['validation_sample_count'] = 0
                pattern['validation_date'] = datetime.now().strftime('%Y-%m-%d')

        return patterns

    def _prepare_data_summary(self, df: pd.DataFrame, limit: int = 20) -> str:
        """å‡†å¤‡æ•°æ®æ‘˜è¦ç”¨äºæç¤ºè¯"""
        sample = df.head(limit)

        summary_lines = []
        for _, row in sample.iterrows():
            line = f"ä»£ç :{row['code']} æ—¥æœŸ:{row['date']} å¼€:{row['open']:.2f} æ”¶:{row['close']:.2f} é«˜:{row['high']:.2f} ä½:{row['low']:.2f} é‡:{row['volume']}"
            if 'day2_close' in row:
                line += f" â†’ æ¬¡æ—¥æ”¶:{row['day2_close']:.2f} â†’ ç¬¬3æ—¥æ”¶:{row['day3_close']:.2f}"
            if 'rise_pct' in row:
                line += f" (æ¶¨å¹…:{row['rise_pct']:.1f}%)"
            summary_lines.append(line)

        return "\n".join(summary_lines)

    def _filter_by_characteristics(self, data: pd.DataFrame, characteristics: List[str]) -> pd.DataFrame:
        """æ ¹æ®ç‰¹å¾ç­›é€‰æ ·æœ¬"""
        import re

        filtered = data.copy()

        # é¢„è®¡ç®—å¸¸ç”¨å­—æ®µï¼ˆå¦‚æœSQLæŸ¥è¯¢æ²¡æœ‰æä¾›ï¼‰
        if 'day_change_pct' not in filtered.columns and 'open' in filtered.columns and 'close' in filtered.columns:
            filtered['day_change_pct'] = ((filtered['close'] - filtered['open']) / filtered['open'] * 100)

        if 'is_yang' not in filtered.columns and 'open' in filtered.columns and 'close' in filtered.columns:
            filtered['is_yang'] = filtered['close'] > filtered['open']
            filtered['is_yin'] = filtered['close'] < filtered['open']

        if 'amplitude' not in filtered.columns and 'high' in filtered.columns and 'low' in filtered.columns:
            filtered['amplitude'] = ((filtered['high'] - filtered['low']) / filtered['low'] * 100)

        for char in characteristics:
            try:
                char_lower = char.lower()

                # é˜³çº¿ç‰¹å¾
                if 'é˜³çº¿' in char and 'is_yang' in filtered.columns:
                    # æå–è¿ç»­å¤©æ•°
                    nums = re.findall(r'è¿ç»­(\d+)', char)
                    if nums:
                        # è¿ç»­é˜³çº¿æš‚æ—¶ç®€åŒ–ä¸ºå•æ—¥é˜³çº¿
                        filtered = filtered[filtered['is_yang'] == True]
                    else:
                        filtered = filtered[filtered['is_yang'] == True]

                # é˜´çº¿ç‰¹å¾
                elif 'é˜´çº¿' in char and 'is_yin' in filtered.columns:
                    filtered = filtered[filtered['is_yin'] == True]

                # æ¶¨å¹…ç‰¹å¾
                elif ('æ¶¨å¹…' in char or 'ä¸Šæ¶¨' in char) and 'day_change_pct' in filtered.columns:
                    nums = re.findall(r'(\d+\.?\d*)[%\-](\d+\.?\d*)', char)
                    if nums:  # èŒƒå›´ï¼š2-4%
                        low, high = float(nums[0][0]), float(nums[0][1])
                        filtered = filtered[(filtered['day_change_pct'] >= low) & (filtered['day_change_pct'] <= high)]
                    else:
                        nums = re.findall(r'[>=<è¶…]+\s*(\d+\.?\d*)', char)
                        if nums:
                            threshold = float(nums[0])
                            if 'è¶…è¿‡' in char or '>' in char or '>=' in char:
                                filtered = filtered[filtered['day_change_pct'] >= threshold]
                            elif '<' in char:
                                filtered = filtered[filtered['day_change_pct'] < threshold]

                # æŒ¯å¹…ç‰¹å¾
                elif 'æŒ¯å¹…' in char and 'amplitude' in filtered.columns:
                    nums = re.findall(r'(\d+\.?\d*)', char)
                    if nums:
                        threshold = float(nums[0])
                        if 'è¶…è¿‡' in char or '>' in char or '>=' in char:
                            filtered = filtered[filtered['amplitude'] > threshold]
                        elif '<' in char or 'å°äº' in char:
                            filtered = filtered[filtered['amplitude'] < threshold]

                # æ¶¨åœç‰¹å¾
                elif 'æ¶¨åœ' in char or 'ä¸€å­—æ¿' in char:
                    if 'day_change_pct' in filtered.columns:
                        filtered = filtered[filtered['day_change_pct'] >= 9.5]

                # æ”¾é‡ç‰¹å¾
                elif 'æ”¾é‡' in char and 'volume' in filtered.columns:
                    nums = re.findall(r'(\d+\.?\d*)[%å€]', char)
                    if nums:
                        multiplier = float(nums[0])
                        if '%' in char:
                            multiplier = multiplier / 100 + 1
                        vol_median = filtered['volume'].median()
                        if vol_median > 0:
                            filtered = filtered[filtered['volume'] >= vol_median * multiplier]
                    else:
                        vol_median = filtered['volume'].median()
                        if vol_median > 0:
                            filtered = filtered[filtered['volume'] > vol_median * 1.2]

                # ç¼©é‡ç‰¹å¾
                elif 'ç¼©é‡' in char and 'volume' in filtered.columns:
                    vol_median = filtered['volume'].median()
                    if vol_median > 0:
                        filtered = filtered[filtered['volume'] < vol_median * 0.8]

                # æ”¶ç›˜ä»·/å¼€ç›˜ä»·å…³ç³»
                elif ('æ”¶ç›˜ä»·æ¥è¿‘å¼€ç›˜ä»·' in char or 'æ”¶ç›˜æ¥è¿‘å¼€ç›˜' in char) and 'day_change_pct' in filtered.columns:
                    filtered = filtered[filtered['day_change_pct'] < 0.5]  # æ¶¨è·Œå¹…<0.5%

                elif ('æ”¶ç›˜ä»·é«˜äºå¼€ç›˜ä»·' in char or 'æ”¶ç›˜é«˜äºå¼€ç›˜' in char):
                    if 'is_yang' in filtered.columns:
                        nums = re.findall(r'(\d+\.?\d*)[%]', char)
                        if nums:
                            threshold = float(nums[0])
                            filtered = filtered[(filtered['is_yang'] == True) & (filtered['day_change_pct'] >= threshold)]
                        else:
                            filtered = filtered[filtered['is_yang'] == True]

                # ä½å¼€ç‰¹å¾
                elif ('ä½å¼€' in char or 'å¼€ç›˜ä»·ä½äº' in char) and 'prev_close' in filtered.columns and 'open' in filtered.columns:
                    nums = re.findall(r'(\d+\.?\d*)[%]', char)
                    if nums:
                        threshold = float(nums[0]) / 100
                        # å¼€ç›˜ä»· < å‰æ”¶ * (1 - threshold)
                        filtered = filtered[filtered['open'] < filtered['prev_close'] * (1 - threshold)]
                    else:
                        # ç®€å•ä½å¼€
                        filtered = filtered[filtered['open'] < filtered['prev_close']]

            except Exception as e:
                print(f"   [è­¦å‘Š] ç‰¹å¾'{char[:30]}'ç­›é€‰å¤±è´¥: {e}")
                continue

        return filtered


    def get_api_statistics(self) -> dict:
        """è·å–APIè°ƒç”¨ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'total_calls': self.api_calls,
            'total_errors': self.api_errors,
            'success_rate': (self.api_calls - self.api_errors) / self.api_calls * 100 if self.api_calls > 0 else 0,
            'input_tokens': self.total_input_tokens,
            'output_tokens': self.total_output_tokens,
            'total_tokens': self.total_input_tokens + self.total_output_tokens,
            'estimated_cost_usd': self._estimate_cost()
        }

    def _estimate_cost(self) -> float:
        """ä¼°ç®—APIè°ƒç”¨æˆæœ¬(ç¾å…ƒ)
        
        åŸºäºClaude APIå®šä»·:
        - Sonnet: $3/M input, $15/M output
        - Haiku: $0.25/M input, $1.25/M output
        """
        if 'haiku' in self.model.lower():
            input_cost = (self.total_input_tokens / 1_000_000) * 0.25
            output_cost = (self.total_output_tokens / 1_000_000) * 1.25
        elif 'sonnet' in self.model.lower():
            input_cost = (self.total_input_tokens / 1_000_000) * 3
            output_cost = (self.total_output_tokens / 1_000_000) * 15
        else:  # opusæˆ–å…¶ä»–
            input_cost = (self.total_input_tokens / 1_000_000) * 15
            output_cost = (self.total_output_tokens / 1_000_000) * 75
        
        return round(input_cost + output_cost, 4)
